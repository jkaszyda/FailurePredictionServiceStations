{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e1154dd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-19T14:45:02.694275Z",
     "end_time": "2023-07-19T14:45:05.418083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read data for X and y\n",
    "X = pd.read_csv(\"data/X.csv\").values\n",
    "y = pd.read_csv(\"data/y.csv\").values\n",
    "\n",
    "# Flatten list y\n",
    "y = [item for sublist in y for item in sublist]\n",
    "\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5574480d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-19T14:45:07.449265Z",
     "end_time": "2023-07-19T14:45:08.400979Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Apply SMOTE to balance the dataset\u001B[39;00m\n\u001B[0;32m      6\u001B[0m sm \u001B[38;5;241m=\u001B[39m SMOTE(k_neighbors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m X_res, y_res \u001B[38;5;241m=\u001B[39m sm\u001B[38;5;241m.\u001B[39mfit_resample(\u001B[43mX\u001B[49m, y)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m#oversampler = RandomOverSampler(random_state=42)\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m#X_res, y_res = oversampler.fit_resample(X,y)\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "sm = SMOTE(k_neighbors=4, random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "#oversampler = RandomOverSampler(random_state=42)\n",
    "#X_res, y_res = oversampler.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0c3147",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-19T14:45:11.294674Z",
     "end_time": "2023-07-19T14:45:13.840500Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot-encoding for sequences in X\n",
    "import numpy as np\n",
    "\n",
    "list_of_sequences = X_res\n",
    "\n",
    "# Create a set of unique IDs\n",
    "unique_ids = set()\n",
    "for sequence in list_of_sequences:\n",
    "    unique_ids.update(sequence)\n",
    "\n",
    "# Convert the set to a sorted list\n",
    "sorted_unique_ids = sorted(unique_ids)\n",
    "\n",
    "# Create a dictionary mapping each ID to its index\n",
    "id_to_index = {id: index for index, id in enumerate(sorted_unique_ids)}\n",
    "\n",
    "# One-hot encode each sequence separately\n",
    "encoded_sequences = []\n",
    "for sequence in list_of_sequences:\n",
    "    encoded_sequence = np.zeros((len(sequence), len(sorted_unique_ids)), dtype=int)\n",
    "    for i, id in enumerate(sequence):\n",
    "        index = id_to_index[id]\n",
    "        encoded_sequence[i, index] = 1\n",
    "    encoded_sequences.append(encoded_sequence)\n",
    "\n",
    "X_res = encoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6fb029",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-19T14:45:19.157009Z",
     "end_time": "2023-07-19T14:45:20.814946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Flatten nested list in order to pass it to the model\n",
    "\n",
    "X_flattened = np.array([np.array(row).flatten() for row in X_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8e34c2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-19T14:45:22.818358Z",
     "end_time": "2023-07-19T14:45:23.307668Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flattened, y_res, random_state = 42, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a91af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#model = RandomForestClassifier(criterion = \"entropy\", n_estimators = 30)\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "#print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165b0ba",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2160 candidates, totalling 6480 fits\n",
      "[CV 1/3; 1/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=10\n",
      "[CV 2/3; 1/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=10\n",
      "[CV 3/3; 1/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=10\n",
      "[CV 1/3; 2/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=20\n",
      "[CV 2/3; 2/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=20\n",
      "[CV 3/3; 2/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=20\n",
      "[CV 1/3; 3/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=30[CV 2/3; 3/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=30\n",
      "\n",
      "[CV 2/3; 2/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=20;, score=nan total time=  34.3s[CV 2/3; 1/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=10;, score=nan total time=  34.4s\n",
      "[CV 2/3; 3/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=30;, score=nan total time=  34.2s\n",
      "[CV 3/3; 3/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=30\n",
      "[CV 1/3; 4/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=40\n",
      "\n",
      "[CV 3/3; 2/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=20;, score=nan total time=  34.9s\n",
      "[CV 1/3; 2/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=20;, score=nan total time=  35.1s[CV 2/3; 4/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=40\n",
      "[CV 3/3; 4/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=40\n",
      "\n",
      "[CV 3/3; 1/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=10;, score=nan total time=  35.1s\n",
      "[CV 1/3; 1/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=10;, score=nan total time=  34.8s\n",
      "[CV 1/3; 5/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=50\n",
      "[CV 2/3; 5/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=50\n",
      "[CV 3/3; 5/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=50\n",
      "[CV 1/3; 3/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=30;, score=nan total time=  36.0s\n",
      "[CV 1/3; 6/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=2, n_estimators=10\n",
      "[CV 3/3; 4/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=40;, score=nan total time=  21.6s\n",
      "[CV 2/3; 6/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=2, n_estimators=10\n",
      "[CV 2/3; 4/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=40;, score=nan total time=  22.8s\n",
      "[CV 3/3; 6/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=2, n_estimators=10\n",
      "[CV 1/3; 4/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=40;, score=nan total time=  23.4s\n",
      "[CV 1/3; 7/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=2, n_estimators=20\n",
      "[CV 3/3; 5/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=50;, score=nan total time=  22.4s\n",
      "[CV 2/3; 7/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=2, n_estimators=20\n",
      "[CV 3/3; 3/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=30;, score=nan total time=  24.2s\n",
      "[CV 3/3; 7/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=2, n_estimators=20\n",
      "[CV 2/3; 5/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=50;, score=nan total time=  23.5s\n",
      "[CV 1/3; 8/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=2, n_estimators=30[CV 1/3; 5/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=1, n_estimators=50;, score=nan total time=  23.9s\n",
      "\n",
      "[CV 2/3; 8/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=2, n_estimators=30\n",
      "[CV 2/3; 6/2160] END criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=2, n_estimators=10;, score=0.128 total time=  49.3s\n",
      "[CV 3/3; 8/2160] START criterion=entropy, max_depth=3, max_features=1, min_samples_leaf=1, min_samples_split=2, n_estimators=30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "grid_space={'max_depth':[3,5,10,None],\n",
    "              'n_estimators':[10,20,30,40,50],\n",
    "              'max_features':[1,3,5,7],\n",
    "              'min_samples_leaf':[1,2,3],\n",
    "              'min_samples_split':[1,2,3],\n",
    "              'criterion': [\"entropy\",\"gini\",\"log_loss\"]\n",
    "           }\n",
    "\n",
    "grid = GridSearchCV(model,param_grid=grid_space,cv=3,scoring=\"f1_micro\",verbose=10, n_jobs=-1)\n",
    "\n",
    "# Enable verbose logging using joblib\n",
    "joblib.parallel_backend('threading')\n",
    "\n",
    "model_grid = grid.fit(X_flattened,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adde3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters are: '+str(model_grid.best_params_))\n",
    "print('Best score is: '+str(model_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a5705",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b89777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
